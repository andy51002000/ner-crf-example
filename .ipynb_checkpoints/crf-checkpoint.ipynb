{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER CRF Example\n",
    "\n",
    "### Install required packages\n",
    "\n",
    "* nltk\n",
    "* sklearn_crfsuite\n",
    "* sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional, if you have installed those python modules, you can skip this stsp.\n",
    "!pip install nltk\n",
    "!pip install sklearn_crfsuite\n",
    "!pip install sklearn\n",
    "!python -m nltk.downloader all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to import those packages into our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#import sklearn\n",
    "import sklearn_crfsuite\n",
    "import pycrfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train.txt', 'test.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list all files under the conll2000 dataset\n",
    "nltk.corpus.conll2000.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the training data and testing data. \n",
    "\n",
    "The data includes a lot of sentences which have been tokenized and tagged in IOB(Inside–outside–beginning) format.\n",
    "\n",
    "\n",
    "```\n",
    "Alex B-PER\n",
    "is O\n",
    "going O\n",
    "to O\n",
    "Los B-LOC\n",
    "Angeles I-LOC\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Confidence', 'NN', 'B-NP'), ('in', 'IN', 'B-PP'), ('the', 'DT', 'B-NP'), ('pound', 'NN', 'I-NP'), ('is', 'VBZ', 'B-VP'), ('widely', 'RB', 'I-VP'), ('expected', 'VBN', 'I-VP'), ('to', 'TO', 'I-VP'), ('take', 'VB', 'I-VP'), ('another', 'DT', 'B-NP'), ('sharp', 'JJ', 'I-NP'), ('dive', 'NN', 'I-NP'), ('if', 'IN', 'B-SBAR'), ('trade', 'NN', 'B-NP'), ('figures', 'NNS', 'I-NP'), ('for', 'IN', 'B-PP'), ('September', 'NNP', 'B-NP'), (',', ',', 'O'), ('due', 'JJ', 'B-ADJP'), ('for', 'IN', 'B-PP'), ('release', 'NN', 'B-NP'), ('tomorrow', 'NN', 'B-NP'), (',', ',', 'O'), ('fail', 'VB', 'B-VP'), ('to', 'TO', 'I-VP'), ('show', 'VB', 'I-VP'), ('a', 'DT', 'B-NP'), ('substantial', 'JJ', 'I-NP'), ('improvement', 'NN', 'I-NP'), ('from', 'IN', 'B-PP'), ('July', 'NNP', 'B-NP'), ('and', 'CC', 'I-NP'), ('August', 'NNP', 'I-NP'), (\"'s\", 'POS', 'B-NP'), ('near-record', 'JJ', 'I-NP'), ('deficits', 'NNS', 'I-NP'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "train_sent = list(nltk.corpus.conll2000.iob_sents('train.txt'))\n",
    "test_sent = list(nltk.corpus.conll2000.iob_sents('test.txt'))\n",
    "\n",
    "# see the first sentences\n",
    "print(train_sent[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function that can convert the sentence to the text feature. \n",
    "\n",
    "我們會觀察前後字然後納為特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0] #token\n",
    "    postag = sent[i][1]\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word[-3:]=' + word[-3:],\n",
    "        'word[-2:]=' + word[-2:],\n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "        'word.istitle=%s' % word.istitle(),\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "        'postag=' + postag,\n",
    "        'postag[:2]=' + postag[:2]\n",
    "    ]\n",
    "    \n",
    "    # look up the previous word\n",
    "    if i > 0:\n",
    "        word1 =sent[i - 1][0]\n",
    "        postag1 = sent[i - 1][1]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:word.istitle=%s' % word1.istitle(),\n",
    "            '-1:word.isupper=%s' % word1.isupper(),\n",
    "            '-1:postag=' + postag1,\n",
    "            '-1:postag[:2]=' + postag[:2] #we're interested in normal form \n",
    "        ])\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "        \n",
    "    # loop up the next wrd\n",
    "    if i < len(sent) -1:\n",
    "        word2 = sent[i + 1][0]\n",
    "        postag2 = sent[i + 1][1]\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word2.lower(),\n",
    "            '+1:word.istitle=%s' % word2.istitle(),\n",
    "            '+1:word.isupper=%s' % word2.isupper(),\n",
    "            '+1:postag=' + postag2,\n",
    "            '+1:postag[:2]=' +postag2[:2]\n",
    "        ])\n",
    "    else:\n",
    "        features.append('EOS')\n",
    "    return features\n",
    "    \n",
    "def sent2features(sent):\n",
    "    return [ word2features(sent,i) for i in range(len(sent)) ]\n",
    "    \n",
    "def sent2labels(sent):\n",
    "    return [ label for token, pos, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [ token for token, pos, label in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement our training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-64080275e91b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mall_possible_transitions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     )\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mcrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0msent2features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_sent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "        algorithm='lbfgs',\n",
    "        c1=0.1,\n",
    "        c2=0.1,\n",
    "        max_iterations=100,\n",
    "        all_possible_transitions=True\n",
    "    )\n",
    "crf.fit([ sent2features(s) for s in train_sent], [ sent2labels(s) for s in train_sent])\n",
    "def train():\n",
    "    X_train = [ sent2features(s) for s in train_sent]\n",
    "    Y_train = [ sent2labels(s) for s in train_sent]\n",
    "    \n",
    "    trainer = pycrfsuite.Trainer(verbose=False)\n",
    "    trainer.set_params({\n",
    "        'c1': 1.0,\n",
    "        'c2': 1e-3,\n",
    "        'max_iterations': 50,\n",
    "        'feature.possible_transitions': True\n",
    "    })\n",
    "    \n",
    "    for xseq, yseq in zip(X_train, Y_train):\n",
    "        trainer.append(xseq, yseq)\n",
    "        \n",
    "    trainer.train('mytrain_model')\n",
    "    \n",
    "    %%time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use our model we just trained, we can cal .open() to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    tagger = pycrfsuite.Tagger()\n",
    "    tagger.open('mytrain_model')\n",
    "    example_set = test_sent[3]\n",
    "    print(' '.join(sent2tokens(example_set)), end='\\n\\n')\n",
    "    print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_set))))\n",
    "    print(\"Correct:  \", ' '.join(sent2labels(example_set)))\n",
    "    \n",
    "    y_pred = crf.predict(X_test)\n",
    "    print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the existing contract , Rockwell said , it has already delivered 793 of the shipsets to Boeing .\n",
      "\n",
      "Predicted: B-PP B-NP I-NP I-NP O B-NP B-VP O B-NP B-VP I-VP I-VP B-NP B-PP B-NP I-NP B-PP B-NP O\n",
      "Correct:   B-PP B-NP I-NP I-NP O B-NP B-VP O B-NP B-VP I-VP I-VP B-NP B-PP B-NP I-NP B-PP B-NP O\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-919f7be2ba45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-26a8ca2e57ad>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Correct:  \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent2labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
